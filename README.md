# ðŸ“š LLM Document Agent

A fully local Retrieval-Augmented Generation (RAG) pipeline for answering complex questions from your personal document library using LLMs.

## ðŸš€ Features

* âœ… Load and parse your PDF/EPUB documents with persistent cache
* âœ… Semantic chunking and embedding via multilingual-e5-base
* âœ… Fast retrieval powered by FAISS index
* âœ… LLM-backed question answering using Ollama + Llama3
* âœ… Elegant Streamlit UI with document selection and chunk citation
* âœ… Shell-based full pipeline execution
* âœ… Pythonic, extensible, and fully offline-ready

## ðŸ“¦ Installation

```bash
# Setup environment
conda create -n llm-rag python=3.9 -y
conda activate llm-rag

# Clone the repository
git clone https://github.com/YOUR_USERNAME/LLM_Document_Agent.git
cd LLM_Document_Agent

# Install dependencies
pip install -r requirements.txt

# Pull Ollama model
ollama pull llama3
```

## ðŸ› ï¸ Usage

```bash
# Step 1: Put PDFs/EPUBs into docs/
# Step 2: Run full pipeline
bash run_pipeline.sh

# Step 3: Launch Web UI
streamlit run app/web_ui.py
```

## ðŸ” Example Queries

* What is the main idea behind Walter Benjamin's theory of history?
* How does Hayao Miyazaki reflect on 1984 in his essays?
* According to the AWS Solutions Architect Guide, how is IAM configured?

## ðŸ§  Architecture Overview

1. `parser.py` â€“ parses PDFs/EPUBs with filename-safe cache
2. `chunk_embed.py` â€“ splits, embeds, and builds FAISS index
3. `rag_chain.py` â€“ queries vector DB and constructs prompt
4. `web_ui.py` â€“ Streamlit-based frontend for interactive QA

All components are connected via clean, modular interfaces.

## ðŸ“¸ Screenshot

![Screenshot](./assets/screenshot_ui.png)

## ðŸ“ Directory Structure

```
LLM_Document_Agent/
â”œâ”€â”€ app/
â”‚   â””â”€â”€ web_ui.py
â”œâ”€â”€ ingest/
â”‚   â”œâ”€â”€ parser.py
â”‚   â””â”€â”€ chunk_embed.py
â”œâ”€â”€ query/
â”‚   â””â”€â”€ rag_chain.py
â”œâ”€â”€ llm_utils/
â”‚   â””â”€â”€ indexer.py
â”œâ”€â”€ docs/
â”œâ”€â”€ store/
â”œâ”€â”€ .parsed_cache/
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

## ðŸ” License

MIT License

---

> This repo was built to demonstrate how personal knowledge and logic reasoning can be enhanced locally using cutting-edge LLMs without leaking data to external servers.
