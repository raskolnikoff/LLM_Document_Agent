# ðŸ“š LLM Document Agent

[![Streamlit Cloud](https://static.streamlit.io/badges/streamlit_badge_black_white.svg)](https://llmdocumentagent-fvhqjfvw3vc6mhgrmhdkdw.streamlit.app/)

> The privacy-first, local-first AI assistant for your documents.
> Query, summarize, and reason across your PDF/EPUB library â€“ all on your machine.

---

## ðŸš€ Features

- ðŸ›¡ï¸ 100% offline document RAG (no data ever leaves your device)
- ðŸ“š Supports PDF/EPUB and rapid semantic vector search
- ðŸ¤– Local LLMs (Ollama, Llama3) or cloud/demo fallback (Streamlit Cloud)
- âš¡ Lightning-fast QA for massive knowledge libraries
- ðŸ–¥ï¸ Beautiful UI with Streamlit ([Live Demo](https://llmdocumentagent-fvhqjfvw3vc6mhgrmhdkdw.streamlit.app/))
- ðŸ› ï¸ Easy to extend, hack, and deploy on any OS

---

## ðŸŒŸ Try it in your browser

**[â–¶ï¸ Live Streamlit Cloud Demo](https://llmdocumentagent-fvhqjfvw3vc6mhgrmhdkdw.streamlit.app/)**  
*(Note: Demo disables local LLMs for security. Run locally for full power!)*

---

## â“ Why LLM Document Agent?

Most document Q&A tools require uploading your private files to third-party servers or depend on closed APIs.  
**LLM Document Agent** is fully open-source, 100% offline-ready, and gives you back control over your knowledge and data.

---

## ðŸ“¦ Quick Start

```bash
# 1. Clone this repo
git clone https://github.com/raskolnikoff/LLM_Document_Agent.git
cd LLM_Document_Agent

# 2. Install requirements
pip install -r requirements.txt

# 3. Add your PDF/EPUB files into docs/
# 4. Run the pipeline (to parse/index your files)
python app/parser.py
python app/chunk_embed.py

# 5. Start the UI
streamlit run app/web_ui.py
```

---

## ðŸ—‚ï¸ Directory Structure (flat app/ example)

```
LLM_Document_Agent/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ web_ui.py
â”‚   â”œâ”€â”€ rag_chain.py
â”‚   â”œâ”€â”€ indexer.py
â”‚   â”œâ”€â”€ ...
â”œâ”€â”€ docs/
â”‚   â””â”€â”€ your-books.pdf
â”œâ”€â”€ index.faiss
â”œâ”€â”€ metadatas.pkl
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md
```

---

## ðŸ› ï¸ FAQ / Troubleshooting

**Q. Why doesn't local LLM (Ollama) work on Streamlit Cloud?**  
A. Cloud demos cannot run system-level LLMs for security; run locally for full features!

**Q. I updated my docs â€“ how do I refresh the index?**  
A. Re-run the parser and embedder steps (`python app/parser.py && python app/chunk_embed.py`).

**Q. Can I use with other LLM APIs (OpenAI, Anthropic)?**  
A. Easily hackable â€“ add your API calls to `rag_chain.py`.

---

## ðŸ“– License

MIT License

---

> Built with â¤ï¸ in Japan.  
> OSS contributions, feature requests, and issues welcome!
