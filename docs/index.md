# ðŸ“š LLM Document Agent

[![Streamlit Cloud](https://static.streamlit.io/badges/streamlit_badge_black_white.svg)](https://llmdocumentagent-j7vpndghp2wayqvnfeo2ac.streamlit.app/)

> The privacy-first, open-source, local-first AI assistant for your documents.
> Query, summarize, and reason across your PDF/EPUB library â€” all on your machine.

---

### ðŸš€ Features

* ðŸ›¡ï¸ 100% offline document RAG (private, no cloud upload needed)
* ðŸ“š Supports both PDF & EPUB, with fast semantic vector search
* ðŸ¤– Local LLM support (Ollama, Llama3) â€” fallback to demo mode in Cloud
* âš¡ Lightning-fast QA for massive libraries
* ðŸ–¥ï¸ Modern, responsive UI (Streamlit)
* ðŸŒ Ready for OSS collaboration, extensible/hackable codebase
* ðŸ”— [Live Demo](https://llmdocumentagent-j7vpndghp2wayqvnfeo2ac.streamlit.app/) using public domain books

---

### ðŸŒŸ Try it instantly (no install)

**[â–¶ï¸ Try Live Demo on Streamlit Cloud](https://llmdocumentagent-j7vpndghp2wayqvnfeo2ac.streamlit.app/)**
*(Demo includes pre-indexed public domain books from [Project Gutenberg](https://www.gutenberg.org/). Local LLMs are disabled for security. Try locally for full power!)*

---

### ðŸ“– What makes LLM Document Agent different?

Most doc Q\&A tools force you to upload private data to the cloud, or lock you into a closed AI API.
**LLM Document Agent** is 100% offline, open-source, and puts *you* in control â€” your files, your server, your AI.

---

### ðŸ“¦ Quick Start (Local Use)

```bash
# 1. Clone the repo
git clone https://github.com/raskolnikoff/LLM_Document_Agent.git
cd LLM_Document_Agent

# 2. Install requirements (Python 3.9+ recommended)
pip install -r requirements.txt

# 3. Add your PDF/EPUB files to docs/
#    Or keep the sample public domain files for testing

# 4. Build your vector index
python app/parser.py
python app/chunk_embed.py

# 5. Launch the web UI
streamlit run app/web_ui.py

# 6. (Optional) To use Ollama/LLM locally, start Ollama server:
ollama serve &
```

---

### ðŸ—‚ï¸ Directory Structure

```
LLM_Document_Agent/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ web_ui.py
â”‚   â”œâ”€â”€ rag_chain.py
â”‚   â”œâ”€â”€ indexer.py
â”‚   â””â”€â”€ ...
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ frankenstein.epub
â”‚   â”œâ”€â”€ pride_and_prejudice.epub
â”‚   â””â”€â”€ ... (other demo/public books)
â”œâ”€â”€ index.faiss
â”œâ”€â”€ metadatas.pkl
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md
â”œâ”€â”€ run_pipeline.sh
â””â”€â”€ .gitignore
```

* `docs/` ... Place your PDFs/EPUBs here (demo branch includes public domain books)
* `index.faiss`, `metadatas.pkl` ... Prebuilt vector index and metadata (auto-generated, included for demo)

---

### ðŸŒ About the Demo

This repo includes a â€œdemoâ€ branch preloaded with [Project Gutenberg](https://www.gutenberg.org/) public domain books (no copyright restrictions).
**You can try the web app instantly on Streamlit Cloud, with no setup â€” just open [the demo link](https://llmdocumentagent-j7vpndghp2wayqvnfeo2ac.streamlit.app/)!**

* All demo books are legal to redistribute and analyze for OSS/AI purposes.
* For your private docs, use the â€œmainâ€ branch locally for full privacy & LLM power.

---

### ðŸ› ï¸ FAQ / Troubleshooting

**Q. Why doesn't Ollama/LLM work in the Cloud demo?**
A. For security reasons, Streamlit Cloud disables local LLMs and external server calls.
ã€€â†’ Use local mode for full-power QA and private data analysis.

**Q. I added/removed docs â€” how to refresh the search?**
A. Run `python app/parser.py && python app/chunk_embed.py` to rebuild the index.

**Q. I want to add OpenAI/Anthropic or other LLMs!**
A. Codebase is modular â€” add your API logic in `app/rag_chain.py`.

**Q. Error: No documents to select / nothing appears?**
A. Confirm you have at least one valid `.pdf` or `.epub` in `docs/`, and that you rebuilt the index.

---

### ðŸ¤ Contributing

Pull requests, issues, and ideas are super welcome!
Letâ€™s make privacy-first document AI accessible to all.

---

### ðŸ“– License

MIT License

> Built with â¤ï¸ in Japan.
> OSS contributions, feature requests, and issues welcome!
